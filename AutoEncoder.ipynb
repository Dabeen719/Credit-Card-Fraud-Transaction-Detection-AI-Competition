{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
      "metadata": {
        "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
      "metadata": {
        "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
      "metadata": {
        "id": "d13862e3-bb27-47af-9b58-a9fbf804df71"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "dTuVaX0VeJQa"
      },
      "id": "dTuVaX0VeJQa"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj1PY9qWeLnK",
        "outputId": "ffcb0a33-d55d-4ddc-84c3-0607bf8ca846"
      },
      "id": "oj1PY9qWeLnK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive/DACON_신용카드 사기 거래 탐지 AI 경진대회'\n",
        "train_df = pd.read_csv(f'{directory}/data/train.csv') # Train\n",
        "train_df = train_df.drop(columns=['ID'])\n",
        "#display(train_df.head())\n",
        "\n",
        "val_df = pd.read_csv(f'{directory}/data/val.csv') # Validation\n",
        "val_df = val_df.drop(columns=['ID'])\n",
        "#display(val_df.head())\n",
        "\n",
        "test_df = pd.read_csv(f'{directory}/data/test.csv') # test\n",
        "#display(test_df.head())"
      ],
      "metadata": {
        "id": "ZM1HQSd-ePpQ"
      },
      "id": "ZM1HQSd-ePpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
      "metadata": {
        "id": "fc7df3f2-62d0-4499-a46e-47d01699def0"
      },
      "source": [
        "## 하이퍼파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
      "metadata": {
        "id": "c3367399-9798-4e38-967b-fd2320b9a2b2"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 400\n",
        "LR = 1e-2\n",
        "BS = 16384 # 한번의 학습시 대략 7번으로 나누어서 들어감\n",
        "SEED = 41"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df)/16384"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23gYVyPtl8R7",
        "outputId": "a394e36e-6a5d-4af4-a814-49cb67a6e6b1"
      },
      "id": "23gYVyPtl8R7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.9483642578125"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
      "metadata": {
        "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd"
      },
      "source": [
        "## 시드고정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
      "metadata": {
        "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
      "metadata": {
        "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e"
      },
      "source": [
        "## 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
      "metadata": {
        "id": "16fd60a5-24e2-4539-bfd0-1c374a641699"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, eval_mode):\n",
        "        self.df = df\n",
        "        self.eval_mode = eval_mode\n",
        "        if self.eval_mode:\n",
        "            self.labels = self.df['Class'].values\n",
        "            self.df = self.df.drop(columns=['Class']).values\n",
        "        else:\n",
        "            self.df = self.df.values\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.eval_mode:\n",
        "            self.x = self.df[index]\n",
        "            self.y = self.labels[index]\n",
        "            return torch.Tensor(self.x), self.y\n",
        "        else:\n",
        "            self.x = self.df[index]\n",
        "            return torch.Tensor(self.x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d880481-1965-499d-9caa-fdfa8526f789",
      "metadata": {
        "id": "9d880481-1965-499d-9caa-fdfa8526f789"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(df=train_df, eval_mode=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)#, num_workers=6)\n",
        "\n",
        "val_dataset = MyDataset(df = val_df, eval_mode=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False)#, num_workers=6)\n",
        "## 우리의 경우 num_workers = 1로 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39962463-032f-490a-a76d-c03991795f38",
      "metadata": {
        "id": "39962463-032f-490a-a76d-c03991795f38"
      },
      "source": [
        "## 1D AutoEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 차원확장 유추(gan)\n",
        "Generative Adversial Network(GAN)은 훈련데이터의 확률분포를 학습하는 대표적인 생산적 모델 (Genertive Model)로 여러 분야에 활용되고 있다. 최근 모바일 장치를 이용한 포지셔닝 데이터의 대량수집이 가능해지면서 GAN을 활용해 위치데이터(위도,경도)를 생산하는 연구가 있었다. 하지만, 훈련 데이터가 위치데이터와 같이 복잡한 분포를 가지는 저차원 데이터인 경우 GAN의 학습이 불안정해진다는 문제점이 있다. 본 논문은 기본적인 Auto Encoder(AE)를 이용해 위치데이터의 차원을 확장시키는 방법을 제시한다. 실험을 통해, 해당 방법으로 차원이 늘어난 데이터를 GAN에 학습시킨다면 학습 안정에 효과가 있고, 의미있는 학습이 가능하다는 것을 확인하였다."
      ],
      "metadata": {
        "id": "SCbV6c_w02F6"
      },
      "id": "SCbV6c_w02F6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
      "metadata": {
        "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae"
      },
      "outputs": [],
      "source": [
        "## 배치 정규화란 : 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 각 배치별로 평균과 분산을 이용해 정규화하는 것\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.Encoder = nn.Sequential(\n",
        "            nn.Linear(30,16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(),# 일반렐루와 다르게 임계치 보다 작으면 0이 아닌 0.01을 곱함\n",
        "            nn.Linear(16,8),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.Decoder = nn.Sequential(\n",
        "            nn.Linear(8,16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(16,30),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Encoder(x)\n",
        "        x = self.Decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
      "metadata": {
        "id": "122af0aa-a1fd-4595-9488-35761e3cb596"
      },
      "source": [
        "## Train (학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
      "metadata": {
        "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        # Loss Function\n",
        "        self.criterion = nn.L1Loss().to(self.device)\n",
        "\n",
        "    def fit(self, ):\n",
        "        self.model.to(self.device)\n",
        "        best_score = 0\n",
        "        for epoch in range(EPOCHS): #  몇번 학습시킬것인가\n",
        "            self.model.train()\n",
        "            train_loss = []\n",
        "            for x in iter(self.train_loader):\n",
        "                x = x.float().to(self.device) # 인풋데이터\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                _x = self.model(x) # 아웃풋데이터\n",
        "                loss = self.criterion(x, _x)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss.append(loss.item())\n",
        "\n",
        "            score = self.validation(self.model, 0.95)\n",
        "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(score)\n",
        "\n",
        "            if best_score < score:\n",
        "                best_score = score\n",
        "                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
        "\n",
        "    def validation(self, eval_model, thr):\n",
        "        cos = nn.CosineSimilarity(dim=1, eps=1e-6) # 기본값은 1, 1e-8임\n",
        "        eval_model.eval()\n",
        "        pred = []\n",
        "        true = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in iter(self.val_loader):\n",
        "                x = x.float().to(self.device)\n",
        "\n",
        "                _x = self.model(x)\n",
        "                diff = cos(x, _x).cpu().tolist() # 인풋과 아웃풋의 코사인 유사도를 계산함\n",
        "                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "                pred += batch_pred\n",
        "                true += y.tolist()\n",
        "\n",
        "        return f1_score(true, pred, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코사인 유사도 예시\n",
        "input1 = torch.randn(100, 128)\n",
        "input2 = torch.randn(100, 128)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "output = cos(input1, input2)\n",
        "output"
      ],
      "metadata": {
        "id": "rJi_xpdCjzBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e10f87-3b2a-43c5-f701-88ee05615f03"
      },
      "id": "rJi_xpdCjzBM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0389,  0.0564, -0.0141, -0.0582, -0.0576, -0.0354, -0.0445,  0.0563,\n",
              "        -0.0292,  0.1327,  0.0099,  0.0009,  0.0353,  0.0475, -0.1926,  0.0227,\n",
              "         0.0359, -0.1252,  0.1518, -0.0207, -0.0415,  0.0630, -0.0226,  0.1588,\n",
              "         0.1386, -0.0470, -0.0066, -0.0038,  0.0211,  0.0135,  0.0149,  0.0828,\n",
              "         0.1035,  0.0355,  0.1377, -0.0357, -0.0584,  0.0028, -0.1251,  0.0115,\n",
              "         0.0532,  0.0416, -0.0367, -0.0908, -0.0151,  0.0105,  0.0899,  0.2239,\n",
              "         0.0566, -0.0742, -0.1069, -0.0435, -0.0076, -0.1017,  0.1429,  0.1977,\n",
              "         0.1413, -0.0622, -0.1238,  0.0459,  0.0180, -0.0466, -0.0404,  0.0603,\n",
              "        -0.1269,  0.0574,  0.0205, -0.0396,  0.1012, -0.0115,  0.0382,  0.0323,\n",
              "         0.0757,  0.0109, -0.0725,  0.0365,  0.0082,  0.0654,  0.0313, -0.1474,\n",
              "         0.0575, -0.0471,  0.0572,  0.1803,  0.0348,  0.0706, -0.0663, -0.1183,\n",
              "        -0.1083, -0.0072, -0.0803,  0.0861, -0.0229, -0.0129, -0.0106,  0.0460,\n",
              "        -0.0236, -0.0057, -0.0613,  0.0709])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
      "metadata": {
        "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86142d9a-68b7-4d04-8423-49d28025411d",
      "metadata": {
        "tags": [],
        "id": "86142d9a-68b7-4d04-8423-49d28025411d",
        "outputId": "75a23057-f281-4325-c4b6-e6474402b2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [0] Train loss : [0.6721689360482352] Val Score : [0.0010529271374420891])\n",
            "Epoch : [1] Train loss : [0.6229286023548671] Val Score : [0.0010529271374420891])\n",
            "Epoch : [2] Train loss : [0.5939646192959377] Val Score : [0.0010529271374420891])\n",
            "Epoch : [3] Train loss : [0.5610476051058088] Val Score : [0.0010529271374420891])\n",
            "Epoch : [4] Train loss : [0.5299920269421169] Val Score : [0.0010529271374420891])\n",
            "Epoch : [5] Train loss : [0.5058594644069672] Val Score : [0.0016862816792256323])\n",
            "Epoch : [6] Train loss : [0.48562902212142944] Val Score : [0.007002887533176871])\n",
            "Epoch : [7] Train loss : [0.4679757910115378] Val Score : [0.01442751911500396])\n",
            "Epoch : [8] Train loss : [0.4531616270542145] Val Score : [0.016274973944497156])\n",
            "Epoch : [9] Train loss : [0.44054137808935984] Val Score : [0.014872907836955175])\n",
            "Epoch : [10] Train loss : [0.4302045702934265] Val Score : [0.015523140155543836])\n",
            "Epoch : [11] Train loss : [0.4216798799378531] Val Score : [0.01716203221341375])\n",
            "Epoch : [12] Train loss : [0.4135994826044355] Val Score : [0.01991537893764997])\n",
            "Epoch : [13] Train loss : [0.4084664668355669] Val Score : [0.022552274441688287])\n",
            "Epoch : [14] Train loss : [0.40230258873530794] Val Score : [0.02628052044800804])\n",
            "Epoch : [15] Train loss : [0.39848076019968304] Val Score : [0.02921618916453451])\n",
            "Epoch : [16] Train loss : [0.3928052399839674] Val Score : [0.03160502742182675])\n",
            "Epoch : [17] Train loss : [0.38785518067223684] Val Score : [0.03189862307633542])\n",
            "Epoch : [18] Train loss : [0.38371220231056213] Val Score : [0.03259264257752996])\n",
            "Epoch : [19] Train loss : [0.37818150435175213] Val Score : [0.03631018955126571])\n",
            "Epoch : [20] Train loss : [0.37542841264179777] Val Score : [0.03830521197298133])\n",
            "Epoch : [21] Train loss : [0.37119188479014803] Val Score : [0.03752126689300137])\n",
            "Epoch : [22] Train loss : [0.3651514308793204] Val Score : [0.0375866442316907])\n",
            "Epoch : [23] Train loss : [0.35995920641081675] Val Score : [0.03938106345017859])\n",
            "Epoch : [24] Train loss : [0.35472910744803293] Val Score : [0.04009696782451688])\n",
            "Epoch : [25] Train loss : [0.352255037852696] Val Score : [0.04353149338332395])\n",
            "Epoch : [26] Train loss : [0.34637950573648724] Val Score : [0.04959501690729664])\n",
            "Epoch : [27] Train loss : [0.3440223549093519] Val Score : [0.045625499012259645])\n",
            "Epoch : [28] Train loss : [0.34086367913654875] Val Score : [0.05693429145280913])\n",
            "Epoch : [29] Train loss : [0.3385651877948216] Val Score : [0.06295442434726549])\n",
            "Epoch : [30] Train loss : [0.33474081328936983] Val Score : [0.06546721998259732])\n",
            "Epoch : [31] Train loss : [0.33074704238346647] Val Score : [0.07003915967710093])\n",
            "Epoch : [32] Train loss : [0.3292080419404166] Val Score : [0.06025836643702372])\n",
            "Epoch : [33] Train loss : [0.3260056546756199] Val Score : [0.07471800838596451])\n",
            "Epoch : [34] Train loss : [0.32618135639599394] Val Score : [0.07949990655935896])\n",
            "Epoch : [35] Train loss : [0.3247614588056292] Val Score : [0.0754731489575647])\n",
            "Epoch : [36] Train loss : [0.3217758280890329] Val Score : [0.07003915967710093])\n",
            "Epoch : [37] Train loss : [0.32050379684993197] Val Score : [0.08132145553260436])\n",
            "Epoch : [38] Train loss : [0.319479729448046] Val Score : [0.08269035007412584])\n",
            "Epoch : [39] Train loss : [0.3184466276850019] Val Score : [0.08938660202481308])\n",
            "Epoch : [40] Train loss : [0.3168762368815286] Val Score : [0.09035198523782892])\n",
            "Epoch : [41] Train loss : [0.3180239498615265] Val Score : [0.08979640711755701])\n",
            "Epoch : [42] Train loss : [0.31502132756369455] Val Score : [0.09160686094117343])\n",
            "Epoch : [43] Train loss : [0.31492272445133757] Val Score : [0.0914611219596381])\n",
            "Epoch : [44] Train loss : [0.3149100754942213] Val Score : [0.09166514352540392])\n",
            "Epoch : [45] Train loss : [0.3132765122822353] Val Score : [0.09433813241618587])\n",
            "Epoch : [46] Train loss : [0.3115730328219278] Val Score : [0.0960152586002498])\n",
            "Epoch : [47] Train loss : [0.3121474896158491] Val Score : [0.09320692868798532])\n",
            "Epoch : [48] Train loss : [0.31017048018319265] Val Score : [0.09705313289276861])\n",
            "Epoch : [49] Train loss : [0.3118909639971597] Val Score : [0.09725466688206151])\n",
            "Epoch : [50] Train loss : [0.31242131761142183] Val Score : [0.09393238208062235])\n",
            "Epoch : [51] Train loss : [0.31151301094463896] Val Score : [0.10112398510314753])\n",
            "Epoch : [52] Train loss : [0.30987757444381714] Val Score : [0.09915045953832763])\n",
            "Epoch : [53] Train loss : [0.30915867005075726] Val Score : [0.10388388034768437])\n",
            "Epoch : [54] Train loss : [0.3099569337708609] Val Score : [0.10572389378874146])\n",
            "Epoch : [55] Train loss : [0.30932614633015226] Val Score : [0.105469570475336])\n",
            "Epoch : [56] Train loss : [0.3077161950724466] Val Score : [0.10521510344733917])\n",
            "Epoch : [57] Train loss : [0.3080954764570509] Val Score : [0.10040993273093535])\n",
            "Epoch : [58] Train loss : [0.30864391582352774] Val Score : [0.09808864212242333])\n",
            "Epoch : [59] Train loss : [0.30824834534100126] Val Score : [0.10040993273093535])\n",
            "Epoch : [60] Train loss : [0.30675919141088215] Val Score : [0.0995229411109602])\n",
            "Epoch : [61] Train loss : [0.306683429649898] Val Score : [0.09969475249364237])\n",
            "Epoch : [62] Train loss : [0.30612988982881817] Val Score : [0.10498878978492741])\n",
            "Epoch : [63] Train loss : [0.30728081720215933] Val Score : [0.09073172945716311])\n",
            "Epoch : [64] Train loss : [0.3057565007890974] Val Score : [0.10980161712235695])\n",
            "Epoch : [65] Train loss : [0.3061704082148416] Val Score : [0.1116175979121231])\n",
            "Epoch : [66] Train loss : [0.3062019092696054] Val Score : [0.11125499059289731])\n",
            "Epoch : [67] Train loss : [0.3062036761215755] Val Score : [0.1110317014912086])\n",
            "Epoch : [68] Train loss : [0.305276734488351] Val Score : [0.11114335996721245])\n",
            "Epoch : [69] Train loss : [0.3063056766986847] Val Score : [0.10974562386927873])\n",
            "Epoch : [70] Train loss : [0.30543780752590727] Val Score : [0.1123140945099402])\n",
            "Epoch : [71] Train loss : [0.30367293102400644] Val Score : [0.11284270707456165])\n",
            "Epoch : [72] Train loss : [0.3053196966648102] Val Score : [0.1116454786132565])\n",
            "Epoch : [73] Train loss : [0.3052505510193961] Val Score : [0.10868042139844714])\n",
            "Epoch : [74] Train loss : [0.30365330832345144] Val Score : [0.10926939863957037])\n",
            "Epoch : [75] Train loss : [0.3027918892247336] Val Score : [0.1152275769338709])\n",
            "Epoch : [76] Train loss : [0.3038508381162371] Val Score : [0.1166082857408334])\n",
            "Epoch : [77] Train loss : [0.3041188631738935] Val Score : [0.11781977528793487])\n",
            "Epoch : [78] Train loss : [0.3028311814580645] Val Score : [0.11550406139352475])\n",
            "Epoch : [79] Train loss : [0.30214546407972065] Val Score : [0.10634496874393094])\n",
            "Epoch : [80] Train loss : [0.30226234453065054] Val Score : [0.11489556896199186])\n",
            "Epoch : [81] Train loss : [0.3031197658606938] Val Score : [0.11420309048745621])\n",
            "Epoch : [82] Train loss : [0.30259079166821073] Val Score : [0.11531054028991207])\n",
            "Epoch : [83] Train loss : [0.3017563990184239] Val Score : [0.12170717910792316])\n",
            "Epoch : [84] Train loss : [0.30218550137111116] Val Score : [0.12167992193642238])\n",
            "Epoch : [85] Train loss : [0.302176513842174] Val Score : [0.11710429415591732])\n",
            "Epoch : [86] Train loss : [0.30293688603809904] Val Score : [0.11619452253834965])\n",
            "Epoch : [87] Train loss : [0.30070649300302776] Val Score : [0.11935690826859921])\n",
            "Epoch : [88] Train loss : [0.3006443040711539] Val Score : [0.11506160386618983])\n",
            "Epoch : [89] Train loss : [0.2999790438583919] Val Score : [0.11145027720463188])\n",
            "Epoch : [90] Train loss : [0.3015148341655731] Val Score : [0.11927469667953122])\n",
            "Epoch : [91] Train loss : [0.3020950215203421] Val Score : [0.1166082857408334])\n",
            "Epoch : [92] Train loss : [0.3005927630833217] Val Score : [0.1194391046230354])\n",
            "Epoch : [93] Train loss : [0.30097288319042753] Val Score : [0.11428624470347076])\n",
            "Epoch : [94] Train loss : [0.29962589485304697] Val Score : [0.1197403609842996])\n",
            "Epoch 00095: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Epoch : [95] Train loss : [0.3000644104821341] Val Score : [0.12556072310144137])\n",
            "Epoch : [96] Train loss : [0.2992816311972482] Val Score : [0.1256147587027449])\n",
            "Epoch : [97] Train loss : [0.2979972405093057] Val Score : [0.12167992193642238])\n",
            "Epoch : [98] Train loss : [0.29802575281688143] Val Score : [0.12650539022677837])\n",
            "Epoch : [99] Train loss : [0.2979813388415745] Val Score : [0.12739422299031403])\n",
            "Epoch : [100] Train loss : [0.29920139483043123] Val Score : [0.12863021113291917])\n",
            "Epoch : [101] Train loss : [0.29835539204733713] Val Score : [0.12682880997213927])\n",
            "Epoch : [102] Train loss : [0.2984085423605783] Val Score : [0.12820069649549587])\n",
            "Epoch : [103] Train loss : [0.29780361907822744] Val Score : [0.12841550632341853])\n",
            "Epoch : [104] Train loss : [0.29688064541135517] Val Score : [0.12922010849265])\n",
            "Epoch : [105] Train loss : [0.29673001170158386] Val Score : [0.1310384274306514])\n",
            "Epoch : [106] Train loss : [0.2969887171472822] Val Score : [0.13010347003779243])\n",
            "Epoch : [107] Train loss : [0.298429080418178] Val Score : [0.1301302108087497])\n",
            "Epoch : [108] Train loss : [0.299257435968944] Val Score : [0.1279051613636571])\n",
            "Epoch : [109] Train loss : [0.2987502600465502] Val Score : [0.12763632046784074])\n",
            "Epoch : [110] Train loss : [0.2972557970455715] Val Score : [0.12699043073292055])\n",
            "Epoch : [111] Train loss : [0.2973502789224897] Val Score : [0.12967539573319425])\n",
            "Epoch : [112] Train loss : [0.29850307532719206] Val Score : [0.12798578157128154])\n",
            "Epoch : [113] Train loss : [0.297091190304075] Val Score : [0.1259928224166362])\n",
            "Epoch : [114] Train loss : [0.2980222829750606] Val Score : [0.1208068048977848])\n",
            "Epoch : [115] Train loss : [0.2973865440913609] Val Score : [0.12312224176796473])\n",
            "Epoch : [116] Train loss : [0.29807576537132263] Val Score : [0.11578037434312163])\n",
            "Epoch 00117: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Epoch : [117] Train loss : [0.2977502133165087] Val Score : [0.12290483447702516])\n",
            "Epoch : [118] Train loss : [0.2979098515851157] Val Score : [0.12978245349609518])\n",
            "Epoch : [119] Train loss : [0.2973592323916299] Val Score : [0.13093167609681736])\n",
            "Epoch : [120] Train loss : [0.29657259583473206] Val Score : [0.12983597257783727])\n",
            "Epoch : [121] Train loss : [0.29712790676525663] Val Score : [0.13050441039248573])\n",
            "Epoch : [122] Train loss : [0.2962848458971296] Val Score : [0.13306177035237432])\n",
            "Epoch : [123] Train loss : [0.2971468142100743] Val Score : [0.1322109770494212])\n",
            "Epoch : [124] Train loss : [0.29740937267030987] Val Score : [0.1308515955165677])\n",
            "Epoch : [125] Train loss : [0.2970773237092154] Val Score : [0.13071809533769904])\n",
            "Epoch : [126] Train loss : [0.29735618829727173] Val Score : [0.12836181371495542])\n",
            "Epoch : [127] Train loss : [0.29580409611974445] Val Score : [0.13066468387185098])\n",
            "Epoch : [128] Train loss : [0.29732260959489004] Val Score : [0.13260999259983436])\n",
            "Epoch : [129] Train loss : [0.2962540473256792] Val Score : [0.1336987799567683])\n",
            "Epoch : [130] Train loss : [0.2972095438412258] Val Score : [0.13247702787305246])\n",
            "Epoch : [131] Train loss : [0.2968014691557203] Val Score : [0.13372530186972176])\n",
            "Epoch : [132] Train loss : [0.2960984451430185] Val Score : [0.13284922723237788])\n",
            "Epoch : [133] Train loss : [0.2959052835191999] Val Score : [0.13247702787305246])\n",
            "Epoch : [134] Train loss : [0.2980026432446071] Val Score : [0.1329023727049856])\n",
            "Epoch : [135] Train loss : [0.29793531554085867] Val Score : [0.13364573129719357])\n",
            "Epoch : [136] Train loss : [0.29681757943970816] Val Score : [0.13133185947403941])\n",
            "Epoch : [137] Train loss : [0.29660192131996155] Val Score : [0.13409643956096212])\n",
            "Epoch : [138] Train loss : [0.2992609015532902] Val Score : [0.1332211098691218])\n",
            "Epoch : [139] Train loss : [0.2958421622003828] Val Score : [0.13205126882822504])\n",
            "Epoch : [140] Train loss : [0.29571143218449186] Val Score : [0.13141185227305693])\n",
            "Epoch : [141] Train loss : [0.29633926919528414] Val Score : [0.13452021079822205])\n",
            "Epoch : [142] Train loss : [0.2958134710788727] Val Score : [0.13473194214717923])\n",
            "Epoch : [143] Train loss : [0.2962100420679365] Val Score : [0.13260999259983436])\n",
            "Epoch : [144] Train loss : [0.29729041031428743] Val Score : [0.1331945573184832])\n",
            "Epoch : [145] Train loss : [0.29672213537352427] Val Score : [0.1336722564326417])\n",
            "Epoch : [146] Train loss : [0.29700144273894175] Val Score : [0.13404343920765852])\n",
            "Epoch : [147] Train loss : [0.29604778970990864] Val Score : [0.1350229050032206])\n",
            "Epoch : [148] Train loss : [0.29653034465653555] Val Score : [0.13343347218821827])\n",
            "Epoch : [149] Train loss : [0.2952143507344382] Val Score : [0.1329023727049856])\n",
            "Epoch : [150] Train loss : [0.2950338934149061] Val Score : [0.13340693254419486])\n",
            "Epoch : [151] Train loss : [0.2965172401496342] Val Score : [0.13420242096136534])\n",
            "Epoch : [152] Train loss : [0.2957920389516013] Val Score : [0.13356614622143567])\n",
            "Epoch : [153] Train loss : [0.29619889174188885] Val Score : [0.13412293732411626])\n",
            "Epoch : [154] Train loss : [0.2970083611352103] Val Score : [0.13303520811552783])\n",
            "Epoch : [155] Train loss : [0.29591958437647137] Val Score : [0.13473194214717923])\n",
            "Epoch : [156] Train loss : [0.29812271680150715] Val Score : [0.1332211098691218])\n",
            "Epoch : [157] Train loss : [0.2955306683267866] Val Score : [0.13430837662806136])\n",
            "Epoch : [158] Train loss : [0.29540712918554035] Val Score : [0.13338039128736484])\n",
            "Epoch 00159: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Epoch : [159] Train loss : [0.29785443203789846] Val Score : [0.1336987799567683])\n",
            "Epoch : [160] Train loss : [0.29596789819853647] Val Score : [0.13486422206000734])\n",
            "Epoch : [161] Train loss : [0.2946782112121582] Val Score : [0.1350757864858487])\n",
            "Epoch : [162] Train loss : [0.295312408890043] Val Score : [0.13475840133995212])\n",
            "Epoch : [163] Train loss : [0.29582740153585163] Val Score : [0.13497001710549114])\n",
            "Epoch : [164] Train loss : [0.2961430123874119] Val Score : [0.13470548134900734])\n",
            "Epoch : [165] Train loss : [0.29663014837673735] Val Score : [0.1345731532719924])\n",
            "Epoch : [166] Train loss : [0.29670548013278414] Val Score : [0.13467901894528878])\n",
            "Epoch : [167] Train loss : [0.2949713340827397] Val Score : [0.13486422206000734])\n",
            "Epoch : [168] Train loss : [0.29588950106075834] Val Score : [0.13481131490989165])\n",
            "Epoch : [169] Train loss : [0.2951799588544028] Val Score : [0.13491712279148024])\n",
            "Epoch 00170: reducing learning rate of group 0 to 6.2500e-04.\n",
            "Epoch : [170] Train loss : [0.29582405516079496] Val Score : [0.1350757864858487])\n",
            "Epoch : [171] Train loss : [0.29539017166410175] Val Score : [0.1350229050032206])\n",
            "Epoch : [172] Train loss : [0.29401057958602905] Val Score : [0.13518153021052057])\n",
            "Epoch : [173] Train loss : [0.2955088572842734] Val Score : [0.13528724828894273])\n",
            "Epoch : [174] Train loss : [0.29512910331998554] Val Score : [0.13528724828894273])\n",
            "Epoch : [175] Train loss : [0.2949034741946629] Val Score : [0.13528724828894273])\n",
            "Epoch : [176] Train loss : [0.29523911220686777] Val Score : [0.13512866155455547])\n",
            "Epoch : [177] Train loss : [0.2966048674924033] Val Score : [0.1353136738024271])\n",
            "Epoch : [178] Train loss : [0.2951806230204446] Val Score : [0.13523439245492327])\n",
            "Epoch : [179] Train loss : [0.296392023563385] Val Score : [0.13518153021052057])\n",
            "Epoch : [180] Train loss : [0.29403126665524076] Val Score : [0.13557784084311175])\n",
            "Epoch : [181] Train loss : [0.29568261333874296] Val Score : [0.13523439245492327])\n",
            "Epoch : [182] Train loss : [0.2942664112363543] Val Score : [0.13515509668405698])\n",
            "Epoch : [183] Train loss : [0.2955637701920101] Val Score : [0.1349964618563173])\n",
            "Epoch : [184] Train loss : [0.2953442633152008] Val Score : [0.13520796213409347])\n",
            "Epoch : [185] Train loss : [0.29476713708468844] Val Score : [0.13494357075059465])\n",
            "Epoch : [186] Train loss : [0.29604041150638033] Val Score : [0.13497001710549114])\n",
            "Epoch : [187] Train loss : [0.2952183059283665] Val Score : [0.13528724828894273])\n",
            "Epoch : [188] Train loss : [0.2944189054625375] Val Score : [0.13534009771375768])\n",
            "Epoch : [189] Train loss : [0.2953040727547237] Val Score : [0.13497001710549114])\n",
            "Epoch : [190] Train loss : [0.29485719970294405] Val Score : [0.13486422206000734])\n",
            "Epoch : [191] Train loss : [0.2956407240458897] Val Score : [0.13518153021052057])\n",
            "Epoch 00192: reducing learning rate of group 0 to 3.1250e-04.\n",
            "Epoch : [192] Train loss : [0.29616451263427734] Val Score : [0.13523439245492327])\n",
            "Epoch : [193] Train loss : [0.2941347786358425] Val Score : [0.13528724828894273])\n",
            "Epoch : [194] Train loss : [0.2946384378841945] Val Score : [0.13534009771375768])\n",
            "Epoch : [195] Train loss : [0.2953037364142282] Val Score : [0.13539294073054672])\n",
            "Epoch : [196] Train loss : [0.29506836192948477] Val Score : [0.13518153021052057])\n",
            "Epoch : [197] Train loss : [0.2951562660081046] Val Score : [0.13534009771375768])\n",
            "Epoch : [198] Train loss : [0.2951559509549822] Val Score : [0.1353136738024271])\n",
            "Epoch : [199] Train loss : [0.2943969113486154] Val Score : [0.13526082117315721])\n",
            "Epoch : [200] Train loss : [0.29593763606888907] Val Score : [0.13534009771375768])\n",
            "Epoch : [201] Train loss : [0.29408736314092365] Val Score : [0.13512866155455547])\n",
            "Epoch : [202] Train loss : [0.29507854155131746] Val Score : [0.13504934654634854])\n",
            "Epoch 00203: reducing learning rate of group 0 to 1.5625e-04.\n",
            "Epoch : [203] Train loss : [0.2944896136011396] Val Score : [0.13518153021052057])\n",
            "Epoch : [204] Train loss : [0.2941350042819977] Val Score : [0.13534009771375768])\n",
            "Epoch : [205] Train loss : [0.29447183864457266] Val Score : [0.13528724828894273])\n",
            "Epoch : [206] Train loss : [0.2947677671909332] Val Score : [0.13515509668405698])\n",
            "Epoch : [207] Train loss : [0.2958867847919464] Val Score : [0.13518153021052057])\n",
            "Epoch : [208] Train loss : [0.2957728590284075] Val Score : [0.13526082117315721])\n",
            "Epoch : [209] Train loss : [0.2948397397994995] Val Score : [0.13528724828894273])\n",
            "Epoch : [210] Train loss : [0.29506088154656546] Val Score : [0.13536652002308183])\n",
            "Epoch : [211] Train loss : [0.29592087864875793] Val Score : [0.13528724828894273])\n",
            "Epoch : [212] Train loss : [0.29506086451666697] Val Score : [0.1353136738024271])\n",
            "Epoch : [213] Train loss : [0.2946932315826416] Val Score : [0.13518153021052057])\n",
            "Epoch 00214: reducing learning rate of group 0 to 7.8125e-05.\n",
            "Epoch : [214] Train loss : [0.2941832074097225] Val Score : [0.13526082117315721])\n",
            "Epoch : [215] Train loss : [0.29516217964036123] Val Score : [0.13520796213409347])\n",
            "Epoch : [216] Train loss : [0.2964390047958919] Val Score : [0.13526082117315721])\n",
            "Epoch : [217] Train loss : [0.2943803625447409] Val Score : [0.13518153021052057])\n",
            "Epoch : [218] Train loss : [0.2951940340655191] Val Score : [0.13520796213409347])\n",
            "Epoch : [219] Train loss : [0.2940856558935983] Val Score : [0.13523439245492327])\n",
            "Epoch : [220] Train loss : [0.2939959764480591] Val Score : [0.13518153021052057])\n",
            "Epoch : [221] Train loss : [0.2944580742291042] Val Score : [0.13523439245492327])\n",
            "Epoch : [222] Train loss : [0.29449852875300814] Val Score : [0.1351022248218685])\n",
            "Epoch : [223] Train loss : [0.2948149953569685] Val Score : [0.13518153021052057])\n",
            "Epoch : [224] Train loss : [0.29388681054115295] Val Score : [0.13523439245492327])\n",
            "Epoch 00225: reducing learning rate of group 0 to 3.9063e-05.\n",
            "Epoch : [225] Train loss : [0.29478748781340464] Val Score : [0.13528724828894273])\n",
            "Epoch : [226] Train loss : [0.29529488512447905] Val Score : [0.13523439245492327])\n",
            "Epoch : [227] Train loss : [0.295339618410383] Val Score : [0.13518153021052057])\n",
            "Epoch : [228] Train loss : [0.29457039066723417] Val Score : [0.13526082117315721])\n",
            "Epoch : [229] Train loss : [0.29470265337399076] Val Score : [0.13526082117315721])\n",
            "Epoch : [230] Train loss : [0.294200952563967] Val Score : [0.13528724828894273])\n",
            "Epoch : [231] Train loss : [0.29443453890936716] Val Score : [0.13534009771375768])\n",
            "Epoch : [232] Train loss : [0.2943843688283648] Val Score : [0.1353136738024271])\n",
            "Epoch : [233] Train loss : [0.2942852335316794] Val Score : [0.13526082117315721])\n",
            "Epoch : [234] Train loss : [0.2945461869239807] Val Score : [0.13523439245492327])\n",
            "Epoch : [235] Train loss : [0.29404084171567646] Val Score : [0.13523439245492327])\n",
            "Epoch 00236: reducing learning rate of group 0 to 1.9531e-05.\n",
            "Epoch : [236] Train loss : [0.2951191578592573] Val Score : [0.13523439245492327])\n",
            "Epoch : [237] Train loss : [0.29562179105622427] Val Score : [0.13520796213409347])\n",
            "Epoch : [238] Train loss : [0.295469799212047] Val Score : [0.13528724828894273])\n",
            "Epoch : [239] Train loss : [0.2962007096835545] Val Score : [0.1353136738024271])\n",
            "Epoch : [240] Train loss : [0.29608144078935894] Val Score : [0.13523439245492327])\n",
            "Epoch : [241] Train loss : [0.2944818820272173] Val Score : [0.13526082117315721])\n",
            "Epoch : [242] Train loss : [0.29565792850085665] Val Score : [0.13528724828894273])\n",
            "Epoch : [243] Train loss : [0.29431919966425213] Val Score : [0.13528724828894273])\n",
            "Epoch : [244] Train loss : [0.2939604776246207] Val Score : [0.13526082117315721])\n",
            "Epoch : [245] Train loss : [0.29407995513507296] Val Score : [0.13523439245492327])\n",
            "Epoch : [246] Train loss : [0.29550603883607046] Val Score : [0.13528724828894273])\n",
            "Epoch 00247: reducing learning rate of group 0 to 9.7656e-06.\n",
            "Epoch : [247] Train loss : [0.2946616326059614] Val Score : [0.13523439245492327])\n",
            "Epoch : [248] Train loss : [0.2951852721827371] Val Score : [0.13520796213409347])\n",
            "Epoch : [249] Train loss : [0.29466394441468374] Val Score : [0.13528724828894273])\n",
            "Epoch : [250] Train loss : [0.2961692384311131] Val Score : [0.13528724828894273])\n",
            "Epoch : [251] Train loss : [0.2943374216556549] Val Score : [0.13526082117315721])\n",
            "Epoch : [252] Train loss : [0.2952967328684671] Val Score : [0.13520796213409347])\n",
            "Epoch : [253] Train loss : [0.29513760123934063] Val Score : [0.13526082117315721])\n",
            "Epoch : [254] Train loss : [0.2950516768864223] Val Score : [0.13528724828894273])\n",
            "Epoch : [255] Train loss : [0.2950757316180638] Val Score : [0.13523439245492327])\n",
            "Epoch : [256] Train loss : [0.29471519589424133] Val Score : [0.13520796213409347])\n",
            "Epoch : [257] Train loss : [0.2963799067905971] Val Score : [0.13523439245492327])\n",
            "Epoch 00258: reducing learning rate of group 0 to 4.8828e-06.\n",
            "Epoch : [258] Train loss : [0.2952744620186942] Val Score : [0.13523439245492327])\n",
            "Epoch : [259] Train loss : [0.29387669052396503] Val Score : [0.13526082117315721])\n",
            "Epoch : [260] Train loss : [0.2942413943154471] Val Score : [0.13523439245492327])\n",
            "Epoch : [261] Train loss : [0.293896632535117] Val Score : [0.13528724828894273])\n",
            "Epoch : [262] Train loss : [0.2957016314779009] Val Score : [0.13523439245492327])\n",
            "Epoch : [263] Train loss : [0.29410654306411743] Val Score : [0.13526082117315721])\n",
            "Epoch : [264] Train loss : [0.2948525335107531] Val Score : [0.13526082117315721])\n",
            "Epoch : [265] Train loss : [0.2958469177995409] Val Score : [0.13526082117315721])\n",
            "Epoch : [266] Train loss : [0.29633292130061556] Val Score : [0.13523439245492327])\n",
            "Epoch : [267] Train loss : [0.2941693067550659] Val Score : [0.13528724828894273])\n",
            "Epoch : [268] Train loss : [0.2954948629651751] Val Score : [0.13523439245492327])\n",
            "Epoch 00269: reducing learning rate of group 0 to 2.4414e-06.\n",
            "Epoch : [269] Train loss : [0.29533444557871136] Val Score : [0.13526082117315721])\n",
            "Epoch : [270] Train loss : [0.294444420507976] Val Score : [0.13528724828894273])\n",
            "Epoch : [271] Train loss : [0.29503215636525837] Val Score : [0.13528724828894273])\n",
            "Epoch : [272] Train loss : [0.2947953300816672] Val Score : [0.13526082117315721])\n",
            "Epoch : [273] Train loss : [0.29513451031276156] Val Score : [0.13526082117315721])\n",
            "Epoch : [274] Train loss : [0.29441847120012554] Val Score : [0.13526082117315721])\n",
            "Epoch : [275] Train loss : [0.29512965253421236] Val Score : [0.13528724828894273])\n",
            "Epoch : [276] Train loss : [0.29592449750219074] Val Score : [0.13523439245492327])\n",
            "Epoch : [277] Train loss : [0.29579032744680134] Val Score : [0.13523439245492327])\n",
            "Epoch : [278] Train loss : [0.2944834274905069] Val Score : [0.13526082117315721])\n",
            "Epoch : [279] Train loss : [0.2950506125177656] Val Score : [0.13523439245492327])\n",
            "Epoch 00280: reducing learning rate of group 0 to 1.2207e-06.\n",
            "Epoch : [280] Train loss : [0.2950641768319266] Val Score : [0.13523439245492327])\n",
            "Epoch : [281] Train loss : [0.294641307422093] Val Score : [0.13523439245492327])\n",
            "Epoch : [282] Train loss : [0.29459719147000996] Val Score : [0.13528724828894273])\n",
            "Epoch : [283] Train loss : [0.296035430261067] Val Score : [0.13523439245492327])\n",
            "Epoch : [284] Train loss : [0.29568892291613985] Val Score : [0.13526082117315721])\n",
            "Epoch : [285] Train loss : [0.2942835603441511] Val Score : [0.13526082117315721])\n",
            "Epoch : [286] Train loss : [0.2949101286275046] Val Score : [0.13523439245492327])\n",
            "Epoch : [287] Train loss : [0.2951641593660627] Val Score : [0.13526082117315721])\n",
            "Epoch : [288] Train loss : [0.29467082023620605] Val Score : [0.13528724828894273])\n",
            "Epoch : [289] Train loss : [0.2952133502279009] Val Score : [0.13526082117315721])\n",
            "Epoch : [290] Train loss : [0.29471650293895174] Val Score : [0.13526082117315721])\n",
            "Epoch 00291: reducing learning rate of group 0 to 6.1035e-07.\n",
            "Epoch : [291] Train loss : [0.2951101064682007] Val Score : [0.13523439245492327])\n",
            "Epoch : [292] Train loss : [0.29584118723869324] Val Score : [0.13526082117315721])\n",
            "Epoch : [293] Train loss : [0.29481703468731474] Val Score : [0.13523439245492327])\n",
            "Epoch : [294] Train loss : [0.29400759509631563] Val Score : [0.1353136738024271])\n",
            "Epoch : [295] Train loss : [0.2953991847378867] Val Score : [0.13526082117315721])\n",
            "Epoch : [296] Train loss : [0.2945231114115034] Val Score : [0.13528724828894273])\n",
            "Epoch : [297] Train loss : [0.294194404567991] Val Score : [0.13526082117315721])\n",
            "Epoch : [298] Train loss : [0.294220860515322] Val Score : [0.13528724828894273])\n",
            "Epoch : [299] Train loss : [0.2946873349802835] Val Score : [0.13528724828894273])\n",
            "Epoch : [300] Train loss : [0.29446044138499666] Val Score : [0.1353136738024271])\n",
            "Epoch : [301] Train loss : [0.2950992243630545] Val Score : [0.13528724828894273])\n",
            "Epoch 00302: reducing learning rate of group 0 to 3.0518e-07.\n",
            "Epoch : [302] Train loss : [0.2949446439743042] Val Score : [0.13528724828894273])\n",
            "Epoch : [303] Train loss : [0.2945311537810734] Val Score : [0.13528724828894273])\n",
            "Epoch : [304] Train loss : [0.2957362788064139] Val Score : [0.13526082117315721])\n",
            "Epoch : [305] Train loss : [0.2948689545903887] Val Score : [0.1353136738024271])\n",
            "Epoch : [306] Train loss : [0.2949384663786207] Val Score : [0.1353136738024271])\n",
            "Epoch : [307] Train loss : [0.29501235485076904] Val Score : [0.13528724828894273])\n",
            "Epoch : [308] Train loss : [0.2943676837853023] Val Score : [0.13528724828894273])\n",
            "Epoch : [309] Train loss : [0.2958885260990688] Val Score : [0.13528724828894273])\n",
            "Epoch : [310] Train loss : [0.2948889008590153] Val Score : [0.13523439245492327])\n",
            "Epoch : [311] Train loss : [0.2955114756311689] Val Score : [0.13520796213409347])\n",
            "Epoch : [312] Train loss : [0.2952468054635184] Val Score : [0.13528724828894273])\n",
            "Epoch 00313: reducing learning rate of group 0 to 1.5259e-07.\n",
            "Epoch : [313] Train loss : [0.29547515511512756] Val Score : [0.13523439245492327])\n",
            "Epoch : [314] Train loss : [0.29478394985198975] Val Score : [0.13523439245492327])\n",
            "Epoch : [315] Train loss : [0.294977034841265] Val Score : [0.13523439245492327])\n",
            "Epoch : [316] Train loss : [0.29451955216271536] Val Score : [0.13526082117315721])\n",
            "Epoch : [317] Train loss : [0.29416213291031973] Val Score : [0.13523439245492327])\n",
            "Epoch : [318] Train loss : [0.2957145869731903] Val Score : [0.13523439245492327])\n",
            "Epoch : [319] Train loss : [0.2941771532808031] Val Score : [0.13523439245492327])\n",
            "Epoch : [320] Train loss : [0.29491069061415537] Val Score : [0.13520796213409347])\n",
            "Epoch : [321] Train loss : [0.29468114035470144] Val Score : [0.13528724828894273])\n",
            "Epoch : [322] Train loss : [0.29418899757521494] Val Score : [0.13526082117315721])\n",
            "Epoch : [323] Train loss : [0.29511644159044537] Val Score : [0.13526082117315721])\n",
            "Epoch 00324: reducing learning rate of group 0 to 7.6294e-08.\n",
            "Epoch : [324] Train loss : [0.295606119292123] Val Score : [0.1353136738024271])\n",
            "Epoch : [325] Train loss : [0.2946839417730059] Val Score : [0.13528724828894273])\n",
            "Epoch : [326] Train loss : [0.2941483344350542] Val Score : [0.13520796213409347])\n",
            "Epoch : [327] Train loss : [0.29640316111700876] Val Score : [0.13526082117315721])\n",
            "Epoch : [328] Train loss : [0.2939937285014561] Val Score : [0.13523439245492327])\n",
            "Epoch : [329] Train loss : [0.29464599064418245] Val Score : [0.13526082117315721])\n",
            "Epoch : [330] Train loss : [0.29449477791786194] Val Score : [0.13528724828894273])\n",
            "Epoch : [331] Train loss : [0.2956153792994363] Val Score : [0.13526082117315721])\n",
            "Epoch : [332] Train loss : [0.2951325348445347] Val Score : [0.13528724828894273])\n",
            "Epoch : [333] Train loss : [0.29482050452913555] Val Score : [0.13528724828894273])\n",
            "Epoch : [334] Train loss : [0.2951581563268389] Val Score : [0.13526082117315721])\n",
            "Epoch 00335: reducing learning rate of group 0 to 3.8147e-08.\n",
            "Epoch : [335] Train loss : [0.29446087990488323] Val Score : [0.13528724828894273])\n",
            "Epoch : [336] Train loss : [0.2952198939664023] Val Score : [0.13518153021052057])\n",
            "Epoch : [337] Train loss : [0.2951717461858477] Val Score : [0.13523439245492327])\n",
            "Epoch : [338] Train loss : [0.294358674968992] Val Score : [0.13526082117315721])\n",
            "Epoch : [339] Train loss : [0.2944072357245854] Val Score : [0.13520796213409347])\n",
            "Epoch : [340] Train loss : [0.29383060336112976] Val Score : [0.13523439245492327])\n",
            "Epoch : [341] Train loss : [0.29591913308416096] Val Score : [0.13523439245492327])\n",
            "Epoch : [342] Train loss : [0.29622784682682585] Val Score : [0.13523439245492327])\n",
            "Epoch : [343] Train loss : [0.2941831180027553] Val Score : [0.13520796213409347])\n",
            "Epoch : [344] Train loss : [0.29546102455684115] Val Score : [0.13523439245492327])\n",
            "Epoch : [345] Train loss : [0.294645083802087] Val Score : [0.13526082117315721])\n",
            "Epoch 00346: reducing learning rate of group 0 to 1.9073e-08.\n",
            "Epoch : [346] Train loss : [0.2951028985636575] Val Score : [0.13526082117315721])\n",
            "Epoch : [347] Train loss : [0.2939877510070801] Val Score : [0.13520796213409347])\n",
            "Epoch : [348] Train loss : [0.2946001759597233] Val Score : [0.13520796213409347])\n",
            "Epoch : [349] Train loss : [0.2953824869224003] Val Score : [0.13526082117315721])\n",
            "Epoch : [350] Train loss : [0.29400832312447683] Val Score : [0.13526082117315721])\n",
            "Epoch : [351] Train loss : [0.2957781936441149] Val Score : [0.13518153021052057])\n",
            "Epoch : [352] Train loss : [0.2951083353587559] Val Score : [0.13523439245492327])\n",
            "Epoch : [353] Train loss : [0.2965806892939976] Val Score : [0.13523439245492327])\n",
            "Epoch : [354] Train loss : [0.29477886642728535] Val Score : [0.13528724828894273])\n",
            "Epoch : [355] Train loss : [0.294207010950361] Val Score : [0.13528724828894273])\n",
            "Epoch : [356] Train loss : [0.2946934018816267] Val Score : [0.13526082117315721])\n",
            "Epoch : [357] Train loss : [0.2947954663208553] Val Score : [0.13523439245492327])\n",
            "Epoch : [358] Train loss : [0.29464990752083914] Val Score : [0.1353136738024271])\n",
            "Epoch : [359] Train loss : [0.29690653511456083] Val Score : [0.13526082117315721])\n",
            "Epoch : [360] Train loss : [0.29457159979002817] Val Score : [0.13526082117315721])\n",
            "Epoch : [361] Train loss : [0.2944937305791037] Val Score : [0.13528724828894273])\n",
            "Epoch : [362] Train loss : [0.29466450214385986] Val Score : [0.13520796213409347])\n",
            "Epoch : [363] Train loss : [0.2952656362737928] Val Score : [0.13520796213409347])\n",
            "Epoch : [364] Train loss : [0.2951429954596928] Val Score : [0.13526082117315721])\n",
            "Epoch : [365] Train loss : [0.2951847655432565] Val Score : [0.13523439245492327])\n",
            "Epoch : [366] Train loss : [0.29572885377066477] Val Score : [0.13523439245492327])\n",
            "Epoch : [367] Train loss : [0.2945573755673] Val Score : [0.13523439245492327])\n",
            "Epoch : [368] Train loss : [0.2947796412876674] Val Score : [0.13526082117315721])\n",
            "Epoch : [369] Train loss : [0.2958267331123352] Val Score : [0.13523439245492327])\n",
            "Epoch : [370] Train loss : [0.2943530763898577] Val Score : [0.13523439245492327])\n",
            "Epoch : [371] Train loss : [0.2948824337550572] Val Score : [0.13523439245492327])\n",
            "Epoch : [372] Train loss : [0.2949101073401315] Val Score : [0.13523439245492327])\n",
            "Epoch : [373] Train loss : [0.2946390850203378] Val Score : [0.1353136738024271])\n",
            "Epoch : [374] Train loss : [0.2945977236543383] Val Score : [0.13528724828894273])\n",
            "Epoch : [375] Train loss : [0.2950977087020874] Val Score : [0.13523439245492327])\n",
            "Epoch : [376] Train loss : [0.2960366266114371] Val Score : [0.13526082117315721])\n",
            "Epoch : [377] Train loss : [0.2946854659489223] Val Score : [0.13528724828894273])\n",
            "Epoch : [378] Train loss : [0.2941950133868626] Val Score : [0.13526082117315721])\n",
            "Epoch : [379] Train loss : [0.29500365257263184] Val Score : [0.13520796213409347])\n",
            "Epoch : [380] Train loss : [0.2952024255480085] Val Score : [0.13520796213409347])\n",
            "Epoch : [381] Train loss : [0.2947577578680856] Val Score : [0.13523439245492327])\n",
            "Epoch : [382] Train loss : [0.29620301723480225] Val Score : [0.13528724828894273])\n",
            "Epoch : [383] Train loss : [0.29462183373314993] Val Score : [0.13523439245492327])\n",
            "Epoch : [384] Train loss : [0.29502723472458975] Val Score : [0.13523439245492327])\n",
            "Epoch : [385] Train loss : [0.29593890479632784] Val Score : [0.13526082117315721])\n",
            "Epoch : [386] Train loss : [0.295048679624285] Val Score : [0.13523439245492327])\n",
            "Epoch : [387] Train loss : [0.2945752910205296] Val Score : [0.1353136738024271])\n",
            "Epoch : [388] Train loss : [0.29394251108169556] Val Score : [0.13526082117315721])\n",
            "Epoch : [389] Train loss : [0.29501775332859587] Val Score : [0.13534009771375768])\n",
            "Epoch : [390] Train loss : [0.2946873903274536] Val Score : [0.13526082117315721])\n",
            "Epoch : [391] Train loss : [0.2944420576095581] Val Score : [0.13523439245492327])\n",
            "Epoch : [392] Train loss : [0.29410201736858915] Val Score : [0.13528724828894273])\n",
            "Epoch : [393] Train loss : [0.2945365948336465] Val Score : [0.13523439245492327])\n",
            "Epoch : [394] Train loss : [0.2962763437202999] Val Score : [0.13518153021052057])\n",
            "Epoch : [395] Train loss : [0.2958138712814876] Val Score : [0.13523439245492327])\n",
            "Epoch : [396] Train loss : [0.29601686767169405] Val Score : [0.13523439245492327])\n",
            "Epoch : [397] Train loss : [0.2950250208377838] Val Score : [0.13526082117315721])\n",
            "Epoch : [398] Train loss : [0.29428779653140474] Val Score : [0.13526082117315721])\n",
            "Epoch : [399] Train loss : [0.2957225867680141] Val Score : [0.13518153021052057])\n"
          ]
        }
      ],
      "source": [
        "model = nn.DataParallel(AutoEncoder())\n",
        "model.eval()\n",
        "# eval(), train() : eval 모드시 평과 가정에서 사용하지 않을 레이어들의 전원을 끈다\n",
        "# 사용하지 않는 레이어들의 예시 : Dropout, BatchNorm 등등\n",
        "\n",
        "# optimizer : 모델학습시 예측결과의 차이를를 잘 줄일수 있게 해주는 역할\n",
        "# scheduler : 가중치의 보폭인 학습률을 조정하는 역할\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
        "\n",
        "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ee1a4c-afe9-4f3c-a3f6-3bca5eb2109f",
      "metadata": {
        "id": "41ee1a4c-afe9-4f3c-a3f6-3bca5eb2109f"
      },
      "source": [
        "## 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53e6313-382b-4f31-a587-1824c579abb7",
      "metadata": {
        "id": "c53e6313-382b-4f31-a587-1824c579abb7"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder()\n",
        "model.load_state_dict(torch.load('./best_model.pth'))\n",
        "model = nn.DataParallel(model)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65628d5a-dedd-4525-8f9d-ba3f00de9eee",
      "metadata": {
        "id": "65628d5a-dedd-4525-8f9d-ba3f00de9eee"
      },
      "outputs": [],
      "source": [
        "#test_df = pd.read_csv('./test.csv')\n",
        "test_df = test_df.drop(columns=['ID'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, eval_mode):\n",
        "        self.df = df\n",
        "        self.eval_mode = eval_mode\n",
        "        if self.eval_mode:\n",
        "            self.labels = self.df['Class'].values\n",
        "            self.df = self.df.drop(columns=['Class']).values\n",
        "        else:\n",
        "            self.df = self.df.values\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.eval_mode:\n",
        "            self.x = self.df[index]\n",
        "            self.y = self.labels[index]\n",
        "            return torch.Tensor(self.x), self.y\n",
        "        else:\n",
        "            self.x = self.df[index]\n",
        "            return torch.Tensor(self.x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "-aGRza0yvkr7"
      },
      "id": "-aGRza0yvkr7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87c859b-be5a-426b-8a02-08ff5b38f1bc",
      "metadata": {
        "id": "e87c859b-be5a-426b-8a02-08ff5b38f1bc"
      },
      "outputs": [],
      "source": [
        "test_dataset = MyDataset(test_df, False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bb801c-9207-4e2d-a44e-1b86eab8ff6e",
      "metadata": {
        "id": "82bb801c-9207-4e2d-a44e-1b86eab8ff6e"
      },
      "outputs": [],
      "source": [
        "def prediction(model, thr, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for x in iter(test_loader):\n",
        "            x = x.float().to(device)\n",
        "\n",
        "            _x = model(x)\n",
        "\n",
        "            diff = cos(x, _x).cpu().tolist()\n",
        "            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist() # thr = 0.95\n",
        "            pred += batch_pred\n",
        "    return pred\n",
        "\n",
        "def prediction_raw(model, thr, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for x in iter(test_loader):\n",
        "            x = x.float().to(device)\n",
        "            print(x.shape)\n",
        "            print(x)\n",
        "\n",
        "            _x = model(x)\n",
        "\n",
        "            diff = cos(x, _x).cpu().tolist()\n",
        "            print('diff')\n",
        "            print(diff.shape)\n",
        "            #batch_pred = np.where(np.array(diff)<thr, 1,0).tolist() # thr = 0.95\n",
        "            pred += diff\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "len(preds),\n",
        "len(list(filter(lambda x : x == 1, preds))),\n",
        "len(list(filter(lambda x : x == 0, preds))),\n",
        "len(list(filter(lambda x : x == 1, preds)))/len(list(filter(lambda x : x == 0, preds))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1pLwUFptmkX",
        "outputId": "2e390684-3ca9-4c35-c91d-5940e0264755"
      },
      "id": "K1pLwUFptmkX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142503 335 142168 0.0023563671149625797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(random.sample(preds_raw, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaxBLn2ytmmO",
        "outputId": "e1085980-7ff6-4771-d6d5-cf21a65ea62d"
      },
      "id": "FaxBLn2ytmmO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9989621043205261, 0.998663067817688, 0.9990600347518921]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EZIoj6Ttmoi"
      },
      "id": "9EZIoj6Ttmoi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff9df77-6591-441d-a4ce-1a0aacc8f8df",
      "metadata": {
        "id": "7ff9df77-6591-441d-a4ce-1a0aacc8f8df"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv(f'{directory}/data/sample_submission.csv')\n",
        "submit['Class'] = preds\n",
        "submit.to_csv(f'{directory}/submit/autoencoder_down_16_8_4_2_by_sh.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5G0_g8Hdr7eK"
      },
      "id": "5G0_g8Hdr7eK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#자카르트 유사도 예시\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n",
        "\n",
        "list1 = [\"삼성전자\", \"테슬라\", \"LG전자\", \"카카오\", \"펄어비스\"]\n",
        "list2 = [\"삼성전자\", \"카카오\", \"넷마블\", \"현대자동차\", \"셀트리온\"]\n",
        "\n",
        "print('jaccard_similarity : ', jaccard_similarity(list1, list2))\n",
        "출처: https://needjarvis.tistory.com/705 [자비스가 필요해:티스토리]"
      ],
      "metadata": {
        "id": "Df7IRUuPr7gP"
      },
      "id": "Df7IRUuPr7gP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}